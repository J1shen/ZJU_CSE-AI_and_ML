{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "#model_name = 'hfl/chinese-roberta-wwm-ext'     #换用模型\n",
    "#model_name = 'hfl/chinese-bert-wwm-ext'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   162, 10477,  8118, 12725,  8755,  2990,   897,   749,   156,\n",
      "         10986,  7566,  1818,  1920,  7030, 10223,   118,  8205,   118,  9143,\n",
      "          4638,  7564,  6378,  5298,  6427,  6241,  3563,  1798,  5310,  3354,\n",
      "          4638,  3563,  1798,  1469,  6444,  4500,  3427,  3373,   511,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['[CLS]', 't', '##ran', '##s', '##form', '##ers', '提', '供', '了', 'n', '##lp', '领', '域', '大', '量', 'state', '-', 'of', '-', 'art', '的', '预', '训', '练', '语', '言', '模', '型', '结', '构', '的', '模', '型', '和', '调', '用', '框', '架', '。', '[SEP]']\n",
      "2\n",
      "torch.Size([1, 40, 1024]) torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'\n",
    "inputs = tokenizer(sen, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(inputs)\n",
    "print(tokens)\n",
    "outputs = model(**inputs)\n",
    "print(len(outputs))\n",
    "print(outputs[0].shape, outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:46, 217.32it/s]\n",
      "10000it [01:25, 117.38it/s]\n",
      "10000it [01:28, 112.87it/s]\n",
      "10000it [00:38, 262.59it/s]\n",
      "10000it [01:33, 107.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 10000, 10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "data_nums = []\n",
    "maxlen = 100      \n",
    "\n",
    "train_input_ids, train_input_masks, train_input_types, train_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "valid_input_ids, valid_input_masks, valid_input_types, valid_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "test_input_ids, test_input_masks, test_input_types, test_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "\n",
    "with open(\"./data/news/体育.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([1,0,0,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/娱乐.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,1,0,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/社会.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,1,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/科技.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,0,1,0])\n",
    "    \n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/财经.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,0,0,1])\n",
    "    \n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "print(data_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'datatype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X21sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m test_labels\u001b[39m.\u001b[39;49mdatatype\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'datatype'"
     ]
    }
   ],
   "source": [
    "test_labels.datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16  # 如果会出现OOM问题，减小它\n",
    "# 训练集\n",
    "train_data = TensorDataset(torch.LongTensor(train_input_ids), \n",
    "                           torch.LongTensor(train_input_masks), \n",
    "                           torch.LongTensor(train_input_types), \n",
    "                           torch.LongTensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 验证集\n",
    "valid_data = TensorDataset(torch.LongTensor(valid_input_ids), \n",
    "                          torch.LongTensor(valid_input_masks),\n",
    "                          torch.LongTensor(valid_input_types), \n",
    "                          torch.LongTensor(valid_labels))\n",
    "valid_sampler = RandomSampler(valid_data)\n",
    "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "# 测试集（是没有标签的）\n",
    "test_data = TensorDataset(torch.LongTensor(test_input_ids), \n",
    "                          torch.LongTensor(test_input_masks),\n",
    "                          torch.LongTensor(test_input_types))\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,classes=10):\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.model_name = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "        #self.model_name = 'hfl/chinese-roberta-wwm-ext'            #更换不同的模型\n",
    "        #self.model_name = 'hfl/chinese-bert-wwm'\n",
    "        self.model = BertModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.fc = nn.Linear(1024,5)     #全连接层\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        out = self.fc(out_pool)   #  [bs, classes]\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 325527557, Trainable parameters: 325527557\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'         #用cpu进行训练\n",
    "model = Bert_Model().to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "EPOCHS = 5\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(torch.argmax(y, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred1,val_pred2,val_pred3,val_pred4,val_pred5 = [],[],[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            #y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            y_pred = torch.argsort(y_pred,dim=1,descending=True).detach().cpu().numpy()\n",
    "            y_pred1=y_pred[:,0].tolist()\n",
    "            val_pred1.extend(y_pred1)\n",
    "            y_pred2=y_pred[:,1].tolist()\n",
    "            val_pred2.extend(y_pred2)\n",
    "            y_pred3=y_pred[:,2].tolist()\n",
    "            val_pred3.extend(y_pred3)\n",
    "            y_pred4=y_pred[:,3].tolist()\n",
    "            val_pred4.extend(y_pred4)\n",
    "            y_pred5=y_pred[:,4].tolist()\n",
    "            val_pred5.extend(y_pred5)\n",
    "    return val_pred1,val_pred2,val_pred3,val_pred4,val_pred5\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            y_real = map(float,y)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "            #if (idx + 1) % 5 == 0:    # 每5epoch打印结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 5.78 GiB total capacity; 4.33 GiB already allocated; 27.50 MiB free; 4.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39m#model.load_state_dict(torch.load(\"best_bert_model.pth\"))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, valid_loader, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (ids, att, tpe, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=47'>48</a>\u001b[0m     ids, att, tpe, y \u001b[39m=\u001b[39m ids\u001b[39m.\u001b[39mto(device), att\u001b[39m.\u001b[39mto(device), tpe\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)  \n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=48'>49</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(ids, att, tpe)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=49'>50</a>\u001b[0m     y_real \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mfloat\u001b[39m,y)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=50'>51</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred, y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 11\u001b[0m in \u001b[0;36mBert_Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=14'>15</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, attention_mask, token_type_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=15'>16</a>\u001b[0m     out_pool \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]   \u001b[39m# 池化后的输出 [bs, config.hidden_size]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X15sdW50aXRsZWQ%3D?line=16'>17</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out_pool)   \u001b[39m#  [bs, classes]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    612\u001b[0m         hidden_states,\n\u001b[1;32m    613\u001b[0m         attention_mask,\n\u001b[1;32m    614\u001b[0m         layer_head_mask,\n\u001b[1;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    617\u001b[0m         past_key_value,\n\u001b[1;32m    618\u001b[0m         output_attentions,\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:452\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 5.78 GiB total capacity; 4.33 GiB already allocated; 27.50 MiB free; 4.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "model.eval()\n",
    "pred1_test,pred2_test,pred3_test,pred4_test,pred5_test = predict(model, test_loader, DEVICE)\n",
    "real_test = []\n",
    "for label in test_labels:\n",
    "    real_test.append(test_labels.argmax())\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(real_test, pred1_test)))\n",
    "\n",
    "top3 = (accuracy_score(real_test, pred1_test,normalize=False)+accuracy_score(real_test, pred2_test,normalize=False)+accuracy_score(real_test, pred3_test,normalize=False))/len(real_test)\n",
    "print(\"\\n Top3 Accuracy = {} \\n\".format(top3))\n",
    "top5 = (accuracy_score(real_test, pred1_test,normalize=False)+accuracy_score(real_test, pred2_test,normalize=False)+accuracy_score(real_test, pred3_test,normalize=False)+accuracy_score(real_test, pred4_test,normalize=False)+accuracy_score(real_test, pred5_test,normalize=False))/len(real_test)\n",
    "print(\"\\n Top5 Accuracy = {} \\n\".format(top5))\n",
    "print(classification_report(test_labels, pred1_test, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "model.eval()\n",
    "sentence = 'AC米兰官方宣布将签新前锋！巨头密询范博梅尔敲定1　　新浪体育讯　在媒体三天的猜测之后，昨天，俱乐部副主席加利亚尼官方表态，亲口承认了AC米兰(微博)将在冬季引进新前锋的事实　　“我们会在冬季引进一位新前锋，”加利亚尼说，“但只是租借，因为我们要为卡萨诺保留位置。”在人选上，加利亚尼被直接问到了两个名字，这也是昨天记者在AC米兰对卡塔尼亚赛后的新闻发布会上，记者曾经询问过阿莱格里的--皮耶罗和马克西-洛佩斯　　对于皮耶罗，加利亚尼回答：“我们可不想抢劫……”而关于洛佩斯，加利亚尼的话更多一些：“他是个配得上AC米兰的球员，不过我们还有其他的候选。”之后就是关于租借和为卡萨诺保留位置的那番话，其实关于加利亚尼的话，除了一些官方性质的确认，一些问题不可太认真，近日Mediaset传出阿森纳(微博)为帕托准备了4000万的传闻，而4000万也恰恰是此前传出AC米兰为帕托的标价，如果帕托会在明夏离队，也不排除AC米兰会为今年冬天的那个前锋花费转会费　　关于AC米兰希望引进的前锋，Mediaset的巴尔吉吉亚提出的两个名字是德罗巴和博列洛，他说：“AC米兰会买进一个有实力的球员，但不是第二前锋，目前，德罗巴是第一选择，接下来是博列洛，前者一直以来都是AC米兰高层喜欢的球员，加利亚尼将试图说服切尔西(微博)放人，同时说服德罗巴相信AC米兰的计划，因此此前他已经向切尔西高层表示了自己希望离队。　　不过这次巴尔吉吉亚的观点算是“仅供参考”，因为随后《米兰新闻》就披露了在AC米兰对卡塔尼亚比赛中场休息时，布拉伊达找到范博梅尔，向他询问了马克西-洛佩斯的技术情况，并得到了荷兰人肯定的答复。之所以选择范博梅尔，是因为在05/06赛季，他和马克西在巴塞罗那曾经是队友，而如今的洛佩斯，比巴萨(微博)时代又成熟了　　由于与俱乐部关系不佳，卡塔尼亚方面对于洛佩斯去留的问题倒是开放的，昨天总经理洛-摩纳哥就公开表示：“马克西是一个优秀的球员，他配得上豪门球队，对于AC米兰他是一个不坏的选择， 但是我不清楚AC米兰的想法是什么。”洛-摩纳哥的这番表态，被认为是把洛佩斯推向AC米兰　　那么洛佩斯是否是正确的选择呢？前卡塔尼亚的中锋斯皮内西表示：“他有可能成为‘副伊布’，这是真的，他拥有强壮的身体，很好的虎丘能力，适合突前中锋的位置，他只是需要逐渐将潜力变现，相对而言，目前卡塔尼亚双前锋的打法与他的特点不符，但如果是在AC米兰，身边有罗比尼奥，则是一个合适的配置。　　但经纪人瓦尔卡雷吉的观点相反：“如果是我，我就不会买马克西-洛佩斯，我会着力培养艾尔-沙拉维，因为他已经是AC米兰的球员了。　　(沈飞'\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "pred1_test,pred2_test,pred3_test,pred4_test,pred5_test = predict(model, tokens, DEVICE)\n",
    "labels = ['Sports','Entertainment','Society','Technology','Finance']\n",
    "print(labels[pred1_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "837d281b97c32b6fba8a22a51e8bf9f92d63e55ecdb04e291285888e30439b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
