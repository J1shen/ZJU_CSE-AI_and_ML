{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/minirbt-h288 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at hfl/minirbt-h288 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hfl/minirbt-h288'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   162, 10477,  8118, 12725,  8755,  2990,   897,   749,   156,\n",
      "         10986,  7566,  1818,  1920,  7030, 10223,   118,  8205,   118,  9143,\n",
      "          4638,  7564,  6378,  5298,  6427,  6241,  3563,  1798,  5310,  3354,\n",
      "          4638,  3563,  1798,  1469,  6444,  4500,  3427,  3373,   511,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['[CLS]', 't', '##ran', '##s', '##form', '##ers', '提', '供', '了', 'n', '##lp', '领', '域', '大', '量', 'state', '-', 'of', '-', 'art', '的', '预', '训', '练', '语', '言', '模', '型', '结', '构', '的', '模', '型', '和', '调', '用', '框', '架', '。', '[SEP]']\n",
      "2\n",
      "torch.Size([1, 40, 288]) torch.Size([1, 288])\n"
     ]
    }
   ],
   "source": [
    "sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'\n",
    "inputs = tokenizer(sen, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(inputs)\n",
    "print(tokens)\n",
    "outputs = model(**inputs)\n",
    "print(len(outputs))\n",
    "print(outputs[0].shape, outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:45, 220.50it/s]\n",
      "10000it [00:35, 280.59it/s]\n",
      "10000it [00:48, 204.35it/s]\n",
      "10000it [00:38, 260.27it/s]\n",
      "10000it [00:51, 195.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 10000, 10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "data_nums = []\n",
    "maxlen = 100      \n",
    "\n",
    "train_input_ids, train_input_masks, train_input_types, train_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "valid_input_ids, valid_input_masks, valid_input_types, valid_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "test_input_ids, test_input_masks, test_input_types, test_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "\n",
    "with open(\"./data/news/体育.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([1,0,0,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/娱乐.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,1,0,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/社会.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,1,0,0])\n",
    "\n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/科技.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,0,1,0])\n",
    "    \n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "with open(\"./data/news/财经.txt\", encoding='utf-8') as f:\n",
    "    input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        content = unicodedata.normalize('NFKC', line.strip())\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=content, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append([0,0,0,0,1])\n",
    "    \n",
    "    train_input_ids.extend(np.array(input_ids[:int(i*0.8)]))\n",
    "    train_input_types.extend(np.array(input_types[:int(i*0.8)]))\n",
    "    train_input_masks.extend(np.array(input_masks[:int(i*0.8)]))\n",
    "    train_labels.extend(np.array(labels[:int(i*0.8)]))    \n",
    "    valid_input_ids.extend(np.array(input_ids[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_types.extend(np.array(input_types[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_input_masks.extend(np.array(input_masks[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    valid_labels.extend(np.array(labels[int(i*0.8):int(i*0.8)+int(i*0.1)]))\n",
    "    test_input_ids.extend(np.array(input_ids[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_types.extend(np.array(input_types[int(i*0.8)+int(i*0.1):]))\n",
    "    test_input_masks.extend(np.array(input_masks[int(i*0.8)+int(i*0.1):]))\n",
    "    test_labels.extend(np.array(labels[int(i*0.8)+int(i*0.1):]))\n",
    "    data_nums.append(i+1)\n",
    "\n",
    "print(data_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # 如果会出现OOM问题，减小它\n",
    "# 训练集\n",
    "train_data = TensorDataset(torch.LongTensor(train_input_ids), \n",
    "                           torch.LongTensor(train_input_masks), \n",
    "                           torch.LongTensor(train_input_types), \n",
    "                           torch.LongTensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 验证集\n",
    "valid_data = TensorDataset(torch.LongTensor(valid_input_ids), \n",
    "                          torch.LongTensor(valid_input_masks),\n",
    "                          torch.LongTensor(valid_input_types), \n",
    "                          torch.LongTensor(valid_labels))\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "# 测试集（是没有标签的）\n",
    "test_data = TensorDataset(torch.LongTensor(test_input_ids), \n",
    "                          torch.LongTensor(test_input_masks),\n",
    "                          torch.LongTensor(test_input_types))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,classes=10):\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.model_name = 'hfl/minirbt-h288'\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.fc = nn.Linear(288,5)     #全连接层\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        out = self.fc(out_pool)   #  [bs, classes]\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/minirbt-h288 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at hfl/minirbt-h288 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 12312581, Trainable parameters: 12312581\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'         #用cpu进行训练\n",
    "model = Bert_Model().to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "EPOCHS = 5\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(torch.argmax(y, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred1,val_pred2,val_pred3,val_pred4,val_pred5 = [],[],[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            #y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            y_pred = torch.argsort(y_pred,dim=1,descending=True).detach().cpu().numpy()\n",
    "            y_pred1=y_pred[:,0].tolist()\n",
    "            val_pred1.extend(y_pred1)\n",
    "            y_pred2=y_pred[:,1].tolist()\n",
    "            val_pred2.extend(y_pred2)\n",
    "            y_pred3=y_pred[:,2].tolist()\n",
    "            val_pred3.extend(y_pred3)\n",
    "            y_pred4=y_pred[:,3].tolist()\n",
    "            val_pred4.extend(y_pred4)\n",
    "            y_pred5=y_pred[:,4].tolist()\n",
    "            val_pred5.extend(y_pred5)\n",
    "    return val_pred1,val_pred2,val_pred3,val_pred4,val_pred5\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        model.bert.requires_grad = False\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            y_real = torch.tensor(y, dtype=torch.float)\n",
    "            ids, att, tpe, y_real = ids.to(device), att.to(device), tpe.to(device), y_real.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y_real)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "            #if (idx + 1) % 5 == 0:    # 每5epoch打印结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0125/0625 | Loss 1.5898 | Time 15.1652\n",
      "Epoch 0001 | Step 0250/0625 | Loss 1.4381 | Time 30.0125\n",
      "Epoch 0001 | Step 0375/0625 | Loss 1.3051 | Time 45.0762\n",
      "Epoch 0001 | Step 0500/0625 | Loss 1.2276 | Time 60.4980\n",
      "Epoch 0001 | Step 0625/0625 | Loss 1.1772 | Time 77.3218\n",
      "current acc is 0.9508, best acc is 0.9508\n",
      "time costed = 81.2406s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0125/0625 | Loss 0.9637 | Time 18.8141\n",
      "Epoch 0002 | Step 0250/0625 | Loss 0.9665 | Time 37.8149\n",
      "Epoch 0002 | Step 0375/0625 | Loss 0.9636 | Time 58.2125\n",
      "Epoch 0002 | Step 0500/0625 | Loss 0.9622 | Time 80.0383\n",
      "Epoch 0002 | Step 0625/0625 | Loss 0.9605 | Time 102.1820\n",
      "current acc is 0.9510, best acc is 0.9510\n",
      "time costed = 107.1796s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0125/0625 | Loss 0.9497 | Time 23.1653\n",
      "Epoch 0003 | Step 0250/0625 | Loss 0.9493 | Time 45.6125\n",
      "Epoch 0003 | Step 0375/0625 | Loss 0.9486 | Time 68.0133\n",
      "Epoch 0003 | Step 0500/0625 | Loss 0.9476 | Time 90.7932\n",
      "Epoch 0003 | Step 0625/0625 | Loss 0.9465 | Time 111.9668\n",
      "current acc is 0.9644, best acc is 0.9644\n",
      "time costed = 115.90598s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0125/0625 | Loss 0.9398 | Time 18.6415\n",
      "Epoch 0004 | Step 0250/0625 | Loss 0.9391 | Time 41.9944\n",
      "Epoch 0004 | Step 0375/0625 | Loss 0.9387 | Time 65.9308\n",
      "Epoch 0004 | Step 0500/0625 | Loss 0.9385 | Time 89.4833\n",
      "Epoch 0004 | Step 0625/0625 | Loss 0.9386 | Time 112.2625\n",
      "current acc is 0.9666, best acc is 0.9666\n",
      "time costed = 116.62758s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0125/0625 | Loss 0.9363 | Time 19.4862\n",
      "Epoch 0005 | Step 0250/0625 | Loss 0.9348 | Time 42.6466\n",
      "Epoch 0005 | Step 0375/0625 | Loss 0.9340 | Time 64.5098\n",
      "Epoch 0005 | Step 0500/0625 | Loss 0.9339 | Time 86.4505\n",
      "Epoch 0005 | Step 0625/0625 | Loss 0.9341 | Time 108.2475\n",
      "current acc is 0.9682, best acc is 0.9682\n",
      "time costed = 112.89735s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:03, 23.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.9696606786427145 \n",
      "\n",
      "\n",
      " Top3 Accuracy = 0.9976047904191617 \n",
      "\n",
      "\n",
      " Top5 Accuracy = 1.0 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9850    0.9830    0.9840      1002\n",
      "           1     0.9560    0.9760    0.9659      1002\n",
      "           2     0.9536    0.9651    0.9593      1002\n",
      "           3     0.9784    0.9501    0.9641      1002\n",
      "           4     0.9760    0.9741    0.9750      1002\n",
      "\n",
      "    accuracy                         0.9697      5010\n",
      "   macro avg     0.9698    0.9697    0.9697      5010\n",
      "weighted avg     0.9698    0.9697    0.9697      5010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "model.eval()\n",
    "pred1_test,pred2_test,pred3_test,pred4_test,pred5_test = predict(model, test_loader, DEVICE)\n",
    "real_test = []\n",
    "for label in test_labels:\n",
    "    real_test.append(np.argmax(label, axis=0))\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(real_test, pred1_test)))\n",
    "\n",
    "top3 = (accuracy_score(real_test, pred1_test,normalize=False)+accuracy_score(real_test, pred2_test,normalize=False)+accuracy_score(real_test, pred3_test,normalize=False))/len(real_test)\n",
    "print(\"\\n Top3 Accuracy = {} \\n\".format(top3))\n",
    "top5 = (accuracy_score(real_test, pred1_test,normalize=False)+accuracy_score(real_test, pred2_test,normalize=False)+accuracy_score(real_test, pred3_test,normalize=False)+accuracy_score(real_test, pred4_test,normalize=False)+accuracy_score(real_test, pred5_test,normalize=False))/len(real_test)\n",
    "print(\"\\n Top5 Accuracy = {} \\n\".format(top5))\n",
    "print(classification_report(real_test, pred1_test, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9875e-01, 3.6630e-04, 3.0062e-04, 2.7174e-04, 3.1062e-04]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Sports\n"
     ]
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "sentence = 'AC米兰官方宣布将签新前锋！巨头密询范博梅尔敲定1　　新浪体育讯　在媒体三天的猜测之后，昨天，俱乐部副主席加利亚尼官方表态，亲口承认了AC米兰(微博)将在冬季引进新前锋的事实　　“我们会在冬季引进一位新前锋，”加利亚尼说，“但只是租借，因为我们要为卡萨诺保留位置。”在人选上，加利亚尼被直接问到了两个名字，这也是昨天记者在AC米兰对卡塔尼亚赛后的新闻发布会上，记者曾经询问过阿莱格里的--皮耶罗和马克西-洛佩斯　　对于皮耶罗，加利亚尼回答：“我们可不想抢劫……”而关于洛佩斯，加利亚尼的话更多一些：“他是个配得上AC米兰的球员，不过我们还有其他的候选。”之后就是关于租借和为卡萨诺保留位置的那番话，其实关于加利亚尼的话，除了一些官方性质的确认，一些问题不可太认真，近日Mediaset传出阿森纳(微博)为帕托准备了4000万的传闻，而4000万也恰恰是此前传出AC米兰为帕托的标价，如果帕托会在明夏离队，也不排除AC米兰会为今年冬天的那个前锋花费转会费　　关于AC米兰希望引进的前锋，Mediaset的巴尔吉吉亚提出的两个名字是德罗巴和博列洛，他说：“AC米兰会买进一个有实力的球员，但不是第二前锋，目前，德罗巴是第一选择，接下来是博列洛，前者一直以来都是AC米兰高层喜欢的球员，加利亚尼将试图说服切尔西(微博)放人，同时说服德罗巴相信AC米兰的计划，因此此前他已经向切尔西高层表示了自己希望离队。　　不过这次巴尔吉吉亚的观点算是“仅供参考”，因为随后《米兰新闻》就披露了在AC米兰对卡塔尼亚比赛中场休息时，布拉伊达找到范博梅尔，向他询问了马克西-洛佩斯的技术情况，并得到了荷兰人肯定的答复。之所以选择范博梅尔，是因为在05/06赛季，他和马克西在巴塞罗那曾经是队友，而如今的洛佩斯，比巴萨(微博)时代又成熟了　　由于与俱乐部关系不佳，卡塔尼亚方面对于洛佩斯去留的问题倒是开放的，昨天总经理洛-摩纳哥就公开表示：“马克西是一个优秀的球员，他配得上豪门球队，对于AC米兰他是一个不坏的选择， 但是我不清楚AC米兰的想法是什么。”洛-摩纳哥的这番表态，被认为是把洛佩斯推向AC米兰　　那么洛佩斯是否是正确的选择呢？前卡塔尼亚的中锋斯皮内西表示：“他有可能成为‘副伊布’，这是真的，他拥有强壮的身体，很好的虎丘能力，适合突前中锋的位置，他只是需要逐渐将潜力变现，相对而言，目前卡塔尼亚双前锋的打法与他的特点不符，但如果是在AC米兰，身边有罗比尼奥，则是一个合适的配置。　　但经纪人瓦尔卡雷吉的观点相反：“如果是我，我就不会买马克西-洛佩斯，我会着力培养艾尔-沙拉维，因为他已经是AC米兰的球员了。　　(沈飞'\n",
    "inputs = tokenizer(sentence, return_tensors='pt',max_length = maxlen,padding='max_length', truncation=True).to(DEVICE)\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "labels = ['Sports','Entertainment','Society','Technology','Finance']\n",
    "print(labels[torch.argmax(outputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dict = torch.load('best_bert_model.pth', map_location=\"cpu\")\n",
    "torch.save(state_dict, 'testmodel2.pth', _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/minirbt-h288 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at hfl/minirbt-h288 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "科技\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm \n",
    "# 预测某一篇新闻\n",
    "\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,classes=10):\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.model_name = 'hfl/minirbt-h288'\n",
    "        self.bert = BertModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.fc = nn.Linear(288,5)     #全连接层\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        out = self.fc(out_pool)   #  [bs, classes]\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "def predict(sentence):\n",
    "    with torch.no_grad():\n",
    "        model = Bert_Model()\n",
    "        model_name = 'hfl/minirbt-h288'\n",
    "        DEVICE = 'cpu'\n",
    "        tokenizer = model.tokenizer\n",
    "        model.load_state_dict(torch.load(\"testmodel2.pth\"))\n",
    "        model.eval()\n",
    "        inputs = tokenizer(sentence, return_tensors='pt',max_length = 100,padding='max_length', truncation=True).to(DEVICE)\n",
    "        outputs = model(**inputs)\n",
    "        labels = ['体育','娱乐','社会','科技','经济']\n",
    "        prediction = labels[torch.argmax(outputs)]\n",
    "    return prediction\n",
    "\n",
    "    # 使用一篇新闻做测试\n",
    "text = \"跨国公司欲抄底中国芯片设计产　　孙燕　　在全球半导体行业困境面前，国际半导体巨头将矛头瞄准了本土半导体设计企业　　日前，《第一财经日报》从可靠途径获悉，美国Aptina近日已经悄然并购了上海智多微电子公司的手机软件平台设计部门　　上海智多微电子公司一位内部员工证实了这一消息，该内部员工表示，由于自己也正在办理离职手续，交易金额并未宣布　　成立于2003年9月的智多微电子主要从事移动多媒体应用处理芯片和手机平台解决方案的研发，目前已经开发出9款多媒体应用处理器。智多微电子董事长兼CEO胡祥在去年10月还曾对媒体表示，智多微电子正在试图降低智能手机入市门槛。龙旗、希姆通、天宇朗通、夏新、天时达等16家国产手机厂家都曾是智多微电子的客户　　而Aptina公司是美光科技有限公司2008年初才成立的子公司，为手机制造商提供200万、300万和500万像素CMOS图像传感器，是CMOS成像行业的领先企业　　“智多手机软件平台部门的员工数大概在30人至40人，这部分员工应该会全部转到Aptina公司。”知情人士透露，此前智多微电子的员工数多达200人，今年逐步缩减到100人左右　　该知情人士指出，智多微电子竞争对手联发科技(MTK)手机芯片功能越来越强大，蚕食了智多微电子的客户和其生存空间　　“目前国内很多半导体芯片企业都很缺钱，受金融风暴影响，融资渠道基本关闭。”业内一位资深分析人士指出，虽然中国集成电路产业发展20多年，但至今营业额达到1亿美元的公司很少　　“从2007年10月到2008年的10月，总共有4家本土IC设计公司被外国公司收购，2009年这种并购案例还会增加。”iSuppli中国半导体行业分析师顾文军指出，随着现在资本市场的低迷，而中国半导体上市公司在美国纳斯达克表现均不佳，并购则成了公司的一种出路。被并购的4家本土IC设计公司为：上海杰脉、杭州晶圆微芯、深圳原核、成都威斯达。　　相关报道　　寒流袭来：中国芯片业熬　　2011年中国芯片市场将达2000亿　　美股评论：全球芯片业濒临绝\"\n",
    "print(predict(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "837d281b97c32b6fba8a22a51e8bf9f92d63e55ecdb04e291285888e30439b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
