{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**医学文本多分类实战**</center>\n",
    "### <center>**《计算医疗》课程作业**</center>\n",
    "<center>3200100259 沈骏一 控制科学与工程学院</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们导入在程序中需要的库，未下载的库可以使用pip语法下载  \n",
    "本次实验所用的方法是Bert（即双向Transformer）的强化版，RoBERTa，在Transformer包中已经得到了集成。  \n",
    "本次实验主要是调用函数并调整参数，以满足医学文本多分类任务要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先下载Bert的预训练模型，以及相对应的中文字典  \n",
    "并以‘model’为变量名加载预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简要介绍：  \n",
    "RoBERTa 模型是BERT 的改进版(A Robustly Optimized BERT，即简单粗暴称为强力优化的BERT方法)。  \n",
    "在模型规模、算力和数据上，与BERT相比主要有以下几点改进：\n",
    "1. 更大的模型参数量（论文提供的训练时间来看，模型使用 1024 块 V100 GPU 训练了 1 天的时间）\n",
    "2. 更大bacth size。RoBERTa 在训练过程中使用了更大的bacth size。尝试过从 256 到 8000 不等的bacth size。\n",
    "3. 更多的训练数据（包括：CC-NEWS 等在内的 160GB 纯文本。而最初的BERT使用16GB BookCorpus数据集和英语维基百科进行训练） \n",
    " \n",
    "另外，RoBERTa在训练方法上有以下改进：\n",
    "1. 去掉下一句预测(NSP)任务\n",
    "2. 动态掩码。BERT 依赖随机掩码和预测 token。原版的 BERT 实现在数据预处理期间执行一次掩码，得到一个静态掩码。 而 RoBERTa 使用了动态掩码：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。  \n",
    "3. 文本编码。Byte-Pair Encoding（BPE）是字符级和词级别表征的混合，支持处理自然语言语料库中的众多常见词汇。原版的 BERT 实现使用字符级别的 BPE 词汇，大小为 30K，是在利用启发式分词规则对输入进行预处理之后学得的。Facebook 研究者没有采用这种方式，而是考虑用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。  \n",
    "4. RoBERTa建立在BERT的语言掩蔽策略的基础上，修改BERT中的关键超参数，包括删除BERT的下一个句子训练前目标，以及使用更大的bacth size和学习率进行训练。RoBERTa也接受了比BERT多一个数量级的训练，时间更长。这使得RoBERTa表示能够比BERT更好地推广到下游任务。    \n",
    "\n",
    "经过长时间的训练，本文的模型在GLUE排行榜上的得分为88.5分，与Yang等人(2019)报告的88.4分相当。本文模型在GLUE 9个任务的其中4个上达到了state-of-the-art的水平，分别是：MNLI, QNLI, RTE 和 STS-B。此外，RoBERTa还在SQuAD 和 RACE 排行榜上达到了最高分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "#model_name = 'hfl/chinese-roberta-wwm-ext'     #换用模型\n",
    "#model_name = 'hfl/chinese-bert-wwm-ext'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，我们进行训练集与测试集数据的导入过程  \n",
    "注意，这里的数据已经在‘textprocession.ipynb’中进行了预处理，统一处理成了‘类别ID+\\t+特征描述’的格式，方便进行下一步的处理  \n",
    "接下来的代码将‘train.csv’按8:2的比例划分为训练集与验证集，并放入对应的numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106330it [00:49, 2149.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (20000, 100) (20000, 100) (20000,)\n",
      "(86330, 100) (86330, 100) (86330, 100) (86330,)\n"
     ]
    }
   ],
   "source": [
    "input_ids, input_masks, input_types, labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "valid_input_ids, valid_input_masks, valid_input_types, valid_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "train_input_ids, train_input_masks, train_input_types, train_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "maxlen = 100      # 取30即可覆盖99%\n",
    "divide = 20000\n",
    "with open(\"train.txt\", encoding='utf-8') as f:\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        target, context = line.strip().split('\\t')\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=context, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append(int(target))\n",
    "\n",
    "valid_input_ids, valid_input_types, valid_input_masks = np.array(input_ids[:divide]), np.array(input_types[:divide]), np.array(input_masks[:divide])\n",
    "valid_labels = np.array(labels[:divide])\n",
    "train_input_ids, train_input_types, train_input_masks = np.array(input_ids[divide:]), np.array(input_types[divide:]), np.array(input_masks[divide:])\n",
    "train_labels = np.array(labels[divide:])\n",
    "\n",
    "print(valid_input_ids.shape, valid_input_types.shape, valid_input_masks.shape, valid_labels.shape)\n",
    "print(train_input_ids.shape, train_input_types.shape, train_input_masks.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果展示了处理后的数据集，统一转变成np序列格式，共有id、类别、描述、掩码4个np数组  \n",
    "其中描述长度截断至50字符，这里的maxlen是一个可以进行参数调整的点  \n",
    "共有86330条训练数据、20000条验证数据  \n",
    "我们用同样的方法处理测试集的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26583it [00:12, 2053.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26583, 100) (26583, 100) (26583, 100) (26583,)\n"
     ]
    }
   ],
   "source": [
    "test_input_ids, test_input_masks, test_input_types, test_labels = [], [], [] ,[] # input char ids, segment type ids,  attention mask\n",
    "maxlen = 100      # 取30即可覆盖99%\n",
    "with open(\"test.txt\", encoding='utf-8') as f:\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        target, context = line.strip().split('\\t')\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=context, max_length = maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        test_input_ids.append(encode_dict['input_ids'])\n",
    "        test_input_types.append(encode_dict['token_type_ids'])\n",
    "        test_input_masks.append(encode_dict['attention_mask'])\n",
    "        test_labels.append(int(target))\n",
    "\n",
    "test_input_ids, test_input_types, test_input_masks = np.array(test_input_ids), np.array(test_input_types), np.array(test_input_masks)\n",
    "test_labels = np.array(test_labels)\n",
    "print(test_input_ids.shape, test_input_types.shape, test_input_masks.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一次行堆处理数设置为64，当然，本次实验是在AutoDL平台上借用GPU进行训练的，故可以满足较大显存的需求  \n",
    "将np格式转化成为tensor，便于之后模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # 如果会出现OOM问题，减小它\n",
    "# 训练集\n",
    "train_data = TensorDataset(torch.LongTensor(train_input_ids), \n",
    "                           torch.LongTensor(train_input_masks), \n",
    "                           torch.LongTensor(train_input_types), \n",
    "                           torch.LongTensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 验证集\n",
    "valid_data = TensorDataset(torch.LongTensor(valid_input_ids), \n",
    "                          torch.LongTensor(valid_input_masks),\n",
    "                          torch.LongTensor(valid_input_types), \n",
    "                          torch.LongTensor(valid_labels))\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 测试集（是没有标签的）\n",
    "test_data = TensorDataset(torch.LongTensor(test_input_ids), \n",
    "                          torch.LongTensor(test_input_masks),\n",
    "                          torch.LongTensor(test_input_types))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型，这里的模型是一个BERT模型，之后进行一个池化层与全连接层，预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,classes=10):\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.model_name = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "        #self.model_name = 'hfl/chinese-roberta-wwm-ext'            #更换不同的模型\n",
    "        #self.model_name = 'hfl/chinese-bert-wwm'\n",
    "        self.model = BertModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.fc = nn.Linear(1024,250)     #全连接层\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        logit = self.fc(out_pool)   #  [bs, classes]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型完成后，测试整个模型中的所有参数数量  \n",
    "定义训练的模型是GPU，共进行十次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 325778682, Trainable parameters: 325778682\n"
     ]
    }
   ],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'         #用cpu进行训练\n",
    "EPOCHS = 5          #训练次数\n",
    "model = Bert_Model().to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置优化器为AdmW（当然选择其他类型的优化器也可以）  \n",
    "这里定义了学习率lr，并设置了权重衰减比例，方式模型过拟合  \n",
    "选择warmup，将学习率逐渐上升，防止模型无法收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))\n",
    "# 学习率先线性warmup一个epoch，然后cosine式下降。\n",
    "# 加warmup（学习率从0慢慢升上去），如果把warmup去掉，可能收敛不了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练、验证、测试函数  \n",
    "主要调用的是sklearn中的‘accuracy_score’函数来测试模型的精确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred1,val_pred2,val_pred3,val_pred4,val_pred5 = [],[],[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            #y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            y_pred = torch.argsort(y_pred,dim=1,descending=True).detach().cpu().numpy()\n",
    "            y_pred1=y_pred[:,0].tolist()\n",
    "            val_pred1.extend(y_pred1)\n",
    "            y_pred2=y_pred[:,1].tolist()\n",
    "            val_pred2.extend(y_pred2)\n",
    "            y_pred3=y_pred[:,2].tolist()\n",
    "            val_pred3.extend(y_pred3)\n",
    "            y_pred4=y_pred[:,3].tolist()\n",
    "            val_pred4.extend(y_pred4)\n",
    "            y_pred5=y_pred[:,4].tolist()\n",
    "            val_pred5.extend(y_pred5)\n",
    "    return val_pred1,val_pred2,val_pred3,val_pred4,val_pred5\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "            #if (idx + 1) % 5 == 0:    # 每5epoch打印结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成函数构建，进行模型的训练与评估，训练次数由EPOCH参数决定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0269/1349 | Loss 5.0004 | Time 129.7180\n",
      "Epoch 0001 | Step 0538/1349 | Loss 4.0901 | Time 259.8747\n",
      "Epoch 0001 | Step 0807/1349 | Loss 3.5005 | Time 389.2240\n",
      "Epoch 0001 | Step 1076/1349 | Loss 3.0970 | Time 518.6850\n",
      "Epoch 0001 | Step 1345/1349 | Loss 2.8009 | Time 648.0758\n",
      "current acc is 0.6015, best acc is 0.6015\n",
      "time costed = 701.29101s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0269/1349 | Loss 1.4106 | Time 129.4957\n",
      "Epoch 0002 | Step 0538/1349 | Loss 1.3596 | Time 259.0841\n",
      "Epoch 0002 | Step 0807/1349 | Loss 1.3147 | Time 388.5630\n",
      "Epoch 0002 | Step 1076/1349 | Loss 1.2764 | Time 518.1023\n",
      "Epoch 0002 | Step 1345/1349 | Loss 1.2478 | Time 647.6062\n",
      "current acc is 0.6625, best acc is 0.6625\n",
      "time costed = 700.66823s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0269/1349 | Loss 0.9916 | Time 129.5571\n",
      "Epoch 0003 | Step 0538/1349 | Loss 0.9705 | Time 259.0066\n",
      "Epoch 0003 | Step 0807/1349 | Loss 0.9636 | Time 388.4257\n",
      "Epoch 0003 | Step 1076/1349 | Loss 0.9568 | Time 517.8505\n",
      "Epoch 0003 | Step 1345/1349 | Loss 0.9505 | Time 647.3055\n",
      "current acc is 0.6849, best acc is 0.6849\n",
      "time costed = 700.33081s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0269/1349 | Loss 0.7776 | Time 130.5847\n",
      "Epoch 0004 | Step 0538/1349 | Loss 0.7694 | Time 261.2303\n",
      "Epoch 0004 | Step 0807/1349 | Loss 0.7654 | Time 391.1419\n",
      "Epoch 0004 | Step 1076/1349 | Loss 0.7623 | Time 520.6794\n",
      "Epoch 0004 | Step 1345/1349 | Loss 0.7607 | Time 650.2179\n",
      "current acc is 0.6938, best acc is 0.6938\n",
      "time costed = 703.27655s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0269/1349 | Loss 0.6608 | Time 130.5530\n",
      "Epoch 0005 | Step 0538/1349 | Loss 0.6584 | Time 261.0489\n",
      "Epoch 0005 | Step 0807/1349 | Loss 0.6515 | Time 391.4959\n",
      "Epoch 0005 | Step 1076/1349 | Loss 0.6470 | Time 521.8544\n",
      "Epoch 0005 | Step 1345/1349 | Loss 0.6463 | Time 651.5613\n",
      "current acc is 0.6950, best acc is 0.6950\n",
      "time costed = 705.03941s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载最优模型对‘test.csv’中的数据进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "416it [01:06,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.6968363239664447 \n",
      "\n",
      "\n",
      " Top3 Accuracy = 0.9109205131098822 \n",
      "\n",
      "\n",
      " Top5 Accuracy = 0.9561373810329911 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6167    0.9250    0.7400        40\n",
      "           1     1.0000    0.0500    0.0952        20\n",
      "           2     0.5316    0.5600    0.5455        75\n",
      "           3     0.4158    0.4158    0.4158       101\n",
      "           4     0.6452    0.6818    0.6630        88\n",
      "           5     0.5185    0.4444    0.4786        63\n",
      "           6     0.0000    0.0000    0.0000        47\n",
      "           7     0.6184    0.8294    0.7085       170\n",
      "           8     0.7705    0.9038    0.8319        52\n",
      "           9     0.5414    0.5000    0.5199       144\n",
      "          10     1.0000    0.0833    0.1538        24\n",
      "          11     0.8222    0.8605    0.8409        43\n",
      "          12     0.5455    0.7083    0.6163       144\n",
      "          13     0.8286    0.7632    0.7945        38\n",
      "          14     0.2921    0.3662    0.3250        71\n",
      "          15     0.8045    0.8480    0.8257       296\n",
      "          16     0.8029    0.8485    0.8250       264\n",
      "          17     0.5309    0.7414    0.6187        58\n",
      "          18     0.9000    0.2903    0.4390        31\n",
      "          19     0.7667    0.6053    0.6765        38\n",
      "          20     0.8636    0.8261    0.8444        23\n",
      "          21     0.6364    0.8750    0.7368        16\n",
      "          22     0.7005    0.6851    0.6927       997\n",
      "          23     0.8000    0.5926    0.6809        27\n",
      "          24     0.9459    0.9589    0.9524        73\n",
      "          25     0.6250    0.9375    0.7500        16\n",
      "          26     0.6129    0.5588    0.5846        34\n",
      "          27     0.8333    0.8333    0.8333        54\n",
      "          28     0.0000    0.0000    0.0000        34\n",
      "          29     0.4750    0.4043    0.4368        47\n",
      "          30     0.7704    0.8389    0.8032       180\n",
      "          31     0.9020    0.9262    0.9139       149\n",
      "          32     0.6071    0.5312    0.5667        32\n",
      "          33     0.7333    0.5000    0.5946        44\n",
      "          34     0.9045    0.9253    0.9148      1699\n",
      "          35     0.8033    0.8305    0.8167        59\n",
      "          36     0.8966    0.8966    0.8966        58\n",
      "          37     0.7941    0.7297    0.7606       148\n",
      "          38     0.6238    0.4468    0.5207       141\n",
      "          39     0.5333    0.4948    0.5134        97\n",
      "          40     0.7778    0.7500    0.7636        28\n",
      "          41     0.9062    0.8700    0.8878       100\n",
      "          42     0.7409    0.7500    0.7454       408\n",
      "          43     0.6667    0.2000    0.3077        20\n",
      "          44     0.5385    0.4516    0.4912        93\n",
      "          45     1.0000    0.7931    0.8846        29\n",
      "          46     1.0000    0.0278    0.0541        36\n",
      "          47     0.6099    0.6232    0.6165       138\n",
      "          48     0.8643    0.8832    0.8736       137\n",
      "          49     0.7083    0.6538    0.6800        26\n",
      "          50     0.8098    0.9010    0.8530      1819\n",
      "          51     0.5263    0.5263    0.5263        76\n",
      "          52     0.6154    0.2353    0.3404        34\n",
      "          53     0.3582    0.4364    0.3934        55\n",
      "          54     0.6207    0.4186    0.5000        43\n",
      "          55     0.4667    0.2121    0.2917        33\n",
      "          56     0.4500    0.4888    0.4686       313\n",
      "          57     0.4444    0.0769    0.1311        52\n",
      "          58     0.3404    0.0952    0.1488       168\n",
      "          59     0.0000    0.0000    0.0000        26\n",
      "          60     0.8919    0.9375    0.9141       528\n",
      "          61     0.3125    0.0833    0.1316        60\n",
      "          62     0.8148    0.7857    0.8000        28\n",
      "          63     0.4375    0.3043    0.3590        23\n",
      "          64     0.4762    0.1429    0.2198        70\n",
      "          65     0.5714    0.1600    0.2500        25\n",
      "          66     0.6667    0.3590    0.4667        39\n",
      "          67     0.6839    0.7976    0.7364       331\n",
      "          68     0.2500    0.1190    0.1613        42\n",
      "          69     0.8000    0.5263    0.6349        38\n",
      "          70     0.2500    0.1364    0.1765        66\n",
      "          71     0.5082    0.7949    0.6200        78\n",
      "          72     0.4286    0.5077    0.4648        65\n",
      "          73     0.4679    0.3290    0.3864       155\n",
      "          74     0.6054    0.6364    0.6205       176\n",
      "          75     0.9516    0.8310    0.8872        71\n",
      "          76     0.0000    0.0000    0.0000        47\n",
      "          77     0.4624    0.7477    0.5714       107\n",
      "          78     0.6364    0.5385    0.5833        39\n",
      "          79     0.7769    0.8103    0.7932       116\n",
      "          80     0.5217    0.7500    0.6154        32\n",
      "          81     0.2500    0.0370    0.0645        27\n",
      "          82     0.4755    0.4874    0.4814       199\n",
      "          83     0.9167    0.9706    0.9429        34\n",
      "          84     0.7549    0.7086    0.7310       326\n",
      "          85     0.5862    0.8095    0.6800        21\n",
      "          86     0.7794    0.7361    0.7571        72\n",
      "          87     0.8462    0.8462    0.8462        26\n",
      "          88     0.6000    0.2400    0.3429        25\n",
      "          89     0.4800    0.5217    0.5000        23\n",
      "          90     0.5000    0.1786    0.2632        28\n",
      "          91     0.8571    0.8889    0.8727        27\n",
      "          92     0.5714    0.7143    0.6349        28\n",
      "          93     0.5562    0.6513    0.6000       152\n",
      "          94     0.7016    0.8304    0.7606       572\n",
      "          95     0.5591    0.5712    0.5651       555\n",
      "          96     0.4314    0.4074    0.4190        54\n",
      "          97     0.7011    0.7531    0.7262        81\n",
      "          98     0.6528    0.6267    0.6395        75\n",
      "          99     0.5345    0.5345    0.5345        58\n",
      "         100     0.5596    0.5083    0.5328       120\n",
      "         101     0.6561    0.5533    0.6004       300\n",
      "         102     0.7895    0.6818    0.7317        22\n",
      "         103     0.7222    0.2000    0.3133        65\n",
      "         104     0.5047    0.5753    0.5377       186\n",
      "         105     0.7209    0.9394    0.8158        33\n",
      "         106     0.6800    0.5862    0.6296        29\n",
      "         107     0.9231    0.5714    0.7059        21\n",
      "         108     0.8864    0.8931    0.8897       131\n",
      "         109     0.5000    0.6538    0.5667        26\n",
      "         110     0.6875    0.5238    0.5946        21\n",
      "         111     0.6628    0.6404    0.6514        89\n",
      "         112     0.8235    0.9333    0.8750        30\n",
      "         113     0.8571    0.7500    0.8000        24\n",
      "         114     0.8462    0.8462    0.8462        65\n",
      "         115     0.6500    0.5417    0.5909        24\n",
      "         116     0.5000    0.5441    0.5211        68\n",
      "         117     0.7500    0.3429    0.4706        35\n",
      "         118     0.8788    0.9062    0.8923        32\n",
      "         119     0.6934    0.6463    0.6690       147\n",
      "         120     0.5833    0.4667    0.5185        15\n",
      "         121     0.5858    0.6018    0.5937       658\n",
      "         122     0.7222    0.7222    0.7222        36\n",
      "         123     0.5000    0.3077    0.3810        26\n",
      "         124     0.6739    0.7209    0.6966        43\n",
      "         125     0.6971    0.8356    0.7601       146\n",
      "         126     0.6556    0.5514    0.5990       107\n",
      "         127     0.6944    0.8333    0.7576        30\n",
      "         128     0.7308    0.6333    0.6786        30\n",
      "         129     0.4615    0.2000    0.2791        30\n",
      "         130     0.9310    0.9000    0.9153        30\n",
      "         131     0.8696    0.8000    0.8333        25\n",
      "         132     0.9456    0.9085    0.9267       153\n",
      "         133     0.4918    0.5882    0.5357       102\n",
      "         134     0.6349    0.7692    0.6957        52\n",
      "         135     0.7895    0.6250    0.6977        24\n",
      "         136     0.9600    0.8889    0.9231        27\n",
      "         137     0.6951    0.7917    0.7403        72\n",
      "         138     0.7598    0.7862    0.7728       491\n",
      "         139     0.6200    0.7045    0.6596        44\n",
      "         140     0.5152    0.4474    0.4789        76\n",
      "         141     0.3750    0.8571    0.5217        14\n",
      "         142     0.9000    0.8182    0.8571        22\n",
      "         143     0.5385    0.2593    0.3500        27\n",
      "         144     0.6000    0.1500    0.2400        20\n",
      "         145     0.5870    0.5870    0.5870        46\n",
      "         146     0.4231    0.3929    0.4074        28\n",
      "         147     0.6627    0.6548    0.6587        84\n",
      "         148     0.5000    0.3333    0.4000        30\n",
      "         149     0.8519    0.9079    0.8790        76\n",
      "         150     0.7737    0.7737    0.7737       137\n",
      "         151     0.6774    0.6774    0.6774        31\n",
      "         152     0.0000    0.0000    0.0000        22\n",
      "         153     0.6615    0.7818    0.7167        55\n",
      "         154     0.5143    0.4615    0.4865        39\n",
      "         155     0.4839    0.7143    0.5769        21\n",
      "         156     0.5556    0.2632    0.3571        19\n",
      "         157     0.7317    0.9677    0.8333        62\n",
      "         158     0.7748    0.7452    0.7597       157\n",
      "         159     0.7917    0.8261    0.8085        46\n",
      "         160     0.8000    0.8197    0.8097       122\n",
      "         161     0.5000    0.6087    0.5490        23\n",
      "         162     0.6986    0.6581    0.6777       155\n",
      "         163     0.7273    0.5195    0.6061        77\n",
      "         164     0.5343    0.5487    0.5414       667\n",
      "         165     0.6655    0.7108    0.6874       823\n",
      "         166     0.7391    0.6071    0.6667        28\n",
      "         167     0.3953    0.3953    0.3953        43\n",
      "         168     0.0000    0.0000    0.0000        36\n",
      "         169     0.0000    0.0000    0.0000        30\n",
      "         170     0.6923    0.1071    0.1856        84\n",
      "         171     0.5248    0.9045    0.6642       199\n",
      "         172     0.9059    0.9625    0.9333        80\n",
      "         173     0.8421    0.7273    0.7805        22\n",
      "         174     0.6632    0.7326    0.6961        86\n",
      "         175     0.5909    0.5909    0.5909        22\n",
      "         176     0.7600    0.8693    0.8110       153\n",
      "         177     0.2759    0.1633    0.2051        49\n",
      "         178     0.8409    0.9136    0.8757        81\n",
      "         179     0.7647    0.8667    0.8125        45\n",
      "         180     0.7812    0.9615    0.8621        26\n",
      "         181     0.2000    0.0455    0.0741        22\n",
      "         182     0.5670    0.8792    0.6894       207\n",
      "         183     0.3333    0.0128    0.0247        78\n",
      "         184     0.7283    0.8272    0.7746        81\n",
      "         185     0.6842    0.6842    0.6842        38\n",
      "         186     0.0000    0.0000    0.0000        25\n",
      "         187     0.9545    0.8400    0.8936        25\n",
      "         188     0.0000    0.0000    0.0000        71\n",
      "         189     0.4237    0.5000    0.4587        50\n",
      "         190     0.6111    0.6875    0.6471        16\n",
      "         191     0.6731    0.8974    0.7692        78\n",
      "         192     0.4000    0.1290    0.1951        31\n",
      "         193     0.8824    0.9615    0.9202        78\n",
      "         194     0.7857    0.7857    0.7857        28\n",
      "         195     0.9004    0.8444    0.8715       257\n",
      "         196     0.5952    0.7812    0.6757        32\n",
      "         197     0.7500    0.8750    0.8077        48\n",
      "         198     0.8636    0.9500    0.9048        20\n",
      "         199     0.4375    0.6000    0.5060        35\n",
      "         200     0.4783    0.4074    0.4400        27\n",
      "         201     0.5600    0.4828    0.5185        29\n",
      "         202     0.6479    0.6216    0.6345        74\n",
      "         203     0.6832    0.7709    0.7244       179\n",
      "         204     0.7143    0.8696    0.7843        23\n",
      "         205     0.7429    0.7429    0.7429        70\n",
      "         206     0.7018    0.6780    0.6897        59\n",
      "         207     0.7399    0.7507    0.7453       341\n",
      "         208     0.4306    0.5636    0.4882        55\n",
      "         209     0.5600    0.3889    0.4590        36\n",
      "         210     0.8228    0.7471    0.7831       261\n",
      "         211     0.5760    0.6546    0.6128       359\n",
      "         212     0.6018    0.5484    0.5738       124\n",
      "         213     0.8611    0.7949    0.8267        39\n",
      "         214     0.5814    0.8929    0.7042        56\n",
      "         215     0.7184    0.8409    0.7749        88\n",
      "         216     0.0000    0.0000    0.0000        30\n",
      "         217     0.8000    0.9143    0.8533        35\n",
      "         218     0.6538    0.2656    0.3778        64\n",
      "         219     0.8824    0.9574    0.9184        47\n",
      "         220     0.8571    0.5714    0.6857        21\n",
      "         221     0.8314    0.9256    0.8760       309\n",
      "         222     0.5385    0.5833    0.5600        36\n",
      "         223     0.4300    0.3724    0.3991       239\n",
      "         224     0.4286    0.3158    0.3636        19\n",
      "         225     0.5641    0.4583    0.5057        48\n",
      "         226     0.8772    0.9259    0.9009        54\n",
      "         227     0.7500    0.6818    0.7143        22\n",
      "         228     0.8558    0.8578    0.8568       436\n",
      "         229     0.3448    0.2703    0.3030        37\n",
      "         230     0.8227    0.8992    0.8593       129\n",
      "         231     0.6374    0.7091    0.6714       471\n",
      "         232     0.5000    0.0833    0.1429        24\n",
      "         233     0.5000    0.8333    0.6250        54\n",
      "         234     0.7209    0.5536    0.6263        56\n",
      "         235     0.6250    0.7692    0.6897        52\n",
      "\n",
      "    accuracy                         0.6968     26583\n",
      "   macro avg     0.6333    0.5982    0.5949     26583\n",
      "weighted avg     0.6842    0.6968    0.6817     26583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "pred1_test,pred2_test,pred3_test,pred4_test,pred5_test = predict(model, test_loader, DEVICE)\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(test_labels, pred1_test)))\n",
    "\n",
    "top3 = (accuracy_score(test_labels, pred1_test,normalize=False)+accuracy_score(test_labels, pred2_test,normalize=False)+accuracy_score(test_labels, pred3_test,normalize=False))/len(test_labels)\n",
    "print(\"\\n Top3 Accuracy = {} \\n\".format(top3))\n",
    "top5 = (accuracy_score(test_labels, pred1_test,normalize=False)+accuracy_score(test_labels, pred2_test,normalize=False)+accuracy_score(test_labels, pred3_test,normalize=False)+accuracy_score(test_labels, pred4_test,normalize=False)+accuracy_score(test_labels, pred5_test,normalize=False))/len(test_labels)\n",
    "print(\"\\n Top5 Accuracy = {} \\n\".format(top5))\n",
    "print(classification_report(test_labels, pred1_test, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一些额外的指标数据：**  \n",
    "（尝试了但是好像出了点问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.metrics import precision_score\n",
    "# 传入真实值和预测值\n",
    "result_precision = precision_score(test_labels, pred1_test)\n",
    "# ==3== 计算召回率，查得全不全\n",
    "from sklearn.metrics import recall_score\n",
    "result_recall = recall_score(test_labels, pred1_test)\n",
    "# ==4== F1-score综合评分\n",
    "from sklearn.metrics import f1_score\n",
    "result_f1 = f1_score(test_labels, pred1_test)\n",
    "# 精准率和召回率曲线绘制\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions,recalls,thretholds = precision_recall_curve(test_labels,pred1_test)\n",
    " \n",
    "# 计算平均精准率\n",
    "from sklearn.metrics import average_precision_score\n",
    "# 参数：y_true真实值，y_score预测到的概率\n",
    "precisions_average = average_precision_score(test_labels,pred1_test)\n",
    "import matplotlib.pyplot as plt\n",
    "# 绘图，召回率x轴，精准率y轴\n",
    "fig,axes = plt.subplots(1,2,figsize=(10,5)) #设置画布，1行2列\n",
    "# 在第一张画布上绘图\n",
    "axes[0].plot(precisions,recalls) #横坐标精确率，纵坐标召回率\n",
    "axes[0].set_title(f'平均精准率：{round(precisions_average,2)}')\n",
    "axes[0].set_xlabel('召回率')\n",
    "axes[0].set_ylabel('精准率')\n",
    "# ROC曲线绘制\n",
    "from sklearn.metrics import roc_curve\n",
    "# 传入参数：y_true真实值，y_predict_proba预测到的概率\n",
    "# 产生返回值，FP、TP、阈值\n",
    "fpr,tpr,thretholds = roc_curve(test_labels, pred1_test)\n",
    "# 计算AUC得分\n",
    "from sklearn.metrics import auc\n",
    "# 传入参数：fpr、tpr\n",
    "AUC = auc(fpr,tpr) \n",
    " \n",
    "# 绘图\n",
    "axes[1].plot(fpr,tpr) #传入FP和TP的值\n",
    "axes[1].set_title(f'AUC值为{round(AUC,2)}')\n",
    "axes[1].set_xlabel('FPR')\n",
    "axes[1].set_ylabel('TPR')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当EPOCH = 10，maxlen = 50 时的训练结果：**   \n",
    "![](pics/pic1.png)  \n",
    "![](pics/Screenshot%20from%202022-10-12%2021-02-52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行调参分析：  \n",
    "先调整maxlen的参数，为了方便起见，我们将EPOCH更改为5  \n",
    "**当maxlen = 50**   \n",
    "![](pics/Screenshot%20from%202022-10-18%2000-13-41.png)  \n",
    "![](pics/Screenshot%20from%202022-10-18%2000-16-13.png)  \n",
    "**当maxlen = 100**  \n",
    "![](pics/Screenshot%20from%202022-10-18%2001-32-17.png)  \n",
    "![](pics/Screenshot%20from%202022-10-18%2001-32-28.png)  \n",
    "可以发现，在Maxlen = 50时，已经达到了较好的效果，当然，当Maxlen = 100，时，在第二个小数位有了较小的提升，但训练时间需要两倍左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行学习率的调整：将lr由2e-5降低之1e-5，epoch本次设置10  \n",
    "![](pics/Screenshot%20from%202022-10-19%2003-13-00.png)  \n",
    "![](pics/Screenshot%20from%202022-10-19%2003-13-13.png)  \n",
    "同样进行对比分析，将学习率提升至3e-5，再次进行测试  \n",
    "![](pics/Screenshot%20from%202022-10-20%2001-04-55.png)  \n",
    "![](pics/Screenshot%20from%202022-10-20%2001-05-07.png)  \n",
    "**发现随着学习率的提升，最后的模型准确率有了0.001数量级的提升，但效果并不明显**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试更改网络结构进行测试分析：\n",
    "将原本的线性输出层更改为三层，增加了gelu模块防止过拟合：  \n",
    "![](pics/Screenshot%20from%202022-10-29%2003-54-13.png)   \n",
    "**最终得出结果如下所示：**\n",
    "![](pics/L2hvbWUveGlhbzExLy5jb25maWcvRGluZ1RhbGsvd3Vrb25nLzE4NjI5MTcxMDhfdjIvSW1hZ2VGaWxlcy83MzQ2MTgzLzkyMTkyNDk3NTlfNzIxNTU2NjA3MTFfQzAyRTJFREQtNUREQS00NDk5LTgyQzctMDlDNkYzOEZGQzIxLnBuZw==.png)  \n",
    "![](pics/L2hvbWUveGlhbzExLy5jb25maWcvRGluZ1RhbGsvd3Vrb25nLzE4NjI5MTcxMDhfdjIvSW1hZ2VGaWxlcy83MzQ2MTgzLzkyMTkyNDk3NTlfNzIxNTU1OTM5MTFfNTI4MDFFQkItNDIzRC00RDM4LTg3RTYtOTU2MTk0NjVCQjI5LnBuZw==.png)  \n",
    "![](pics/L2hvbWUveGlhbzExLy5jb25maWcvRGluZ1RhbGsvd3Vrb25nLzE4NjI5MTcxMDhfdjIvSW1hZ2VGaWxlcy83MzQ2MTgzLzkyMTkyNDk3NTlfNzIxNTU0NjE3OTFfRDhEMkMxMkEtMUQ3Ny00REM0LUI1QjYtREYxNDZEMERFNDQwLnBuZw==.png)  \n",
    "**也并没有得到有效提升**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参是个复杂的过程，于是我们尝试用机器学习自动调参  \n",
    "这里使用的时Optuna工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learningrate = trial.suggest_float('x', 1e-5, 1e-4)\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, weight_decay=1e-4) #AdamW优化器\n",
    "    train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)\n",
    "    model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "    pred1_test,pred2_test,pred3_test,pred4_test,pred5_test = predict(model, test_loader, DEVICE)\n",
    "    return accuracy_score(test_labels, pred1_test)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=1000)\n",
    "\n",
    "study.best_params  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是由于计算量过于庞大，本次实验就不尝试了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "在本次实验中，利用了BERT自然语言处理方法，对于长文本分类问题做了一定程度的探究。  \n",
    "在完成模型搭建与分类任务的同时，还探究了参数的选取对于最后实验结果的影响。  \n",
    "虽然在最后可能还是并没有准确完成模型的调参分析，但是经过测试模型的准确率在TOP1、TOP3、TOP5已经可以达到70%、90%、95%  \n",
    "属于是比较好的模型效果了，当然，这也与数据集的结构与选取有关。  \n",
    "总而言之，本次实验给了我很大启发，我也正式迈入了NLP与人工智能的领域。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "837d281b97c32b6fba8a22a51e8bf9f92d63e55ecdb04e291285888e30439b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
